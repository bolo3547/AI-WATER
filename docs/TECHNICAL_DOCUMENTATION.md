# AquaWatch NRW Detection System
## Comprehensive Technical Documentation

**Version:** 2.0  
**Date:** January 2026  
**Authors:** AquaWatch Development Team

---

## Table of Contents

1. [Executive Summary](#1-executive-summary)
2. [System Overview](#2-system-overview)
3. [Technology Stack](#3-technology-stack)
4. [System Architecture](#4-system-architecture)
5. [AI/ML Engine](#5-aiml-engine)
6. [Frontend Dashboard](#6-frontend-dashboard)
7. [Backend API](#7-backend-api)
8. [IoT & Sensor Integration](#8-iot--sensor-integration)
9. [Data Models](#9-data-models)
10. [Deployment](#10-deployment)
11. [API Reference](#11-api-reference)
12. [Appendices](#12-appendices)

---

## 1. Executive Summary

### 1.1 Purpose

The AquaWatch NRW Detection System is an enterprise-grade water utility management platform designed to:

- **Detect** water leaks in real-time using AI-powered anomaly detection
- **Localize** leak positions using gradient-based pressure analysis
- **Predict** potential failures before they occur
- **Reduce** Non-Revenue Water (NRW) losses by up to 40%
- **Optimize** maintenance resource allocation through priority scoring

### 1.2 Key Benefits

| Benefit | Impact |
|---------|--------|
| Water Loss Reduction | 30-40% reduction in NRW |
| Cost Savings | $500K+ annual savings for mid-size utilities |
| Response Time | 80% faster leak detection |
| Operational Efficiency | 60% reduction in manual monitoring |
| Compliance | IWA (International Water Association) aligned metrics |

### 1.3 Target Users

- **Water Utility Operators** - Daily monitoring and incident response
- **Network Engineers** - DMA analysis and infrastructure planning
- **Executive Management** - KPI dashboards and ROI tracking
- **Field Technicians** - Work order management and leak repairs

---

## 2. System Overview

### 2.1 What is Non-Revenue Water (NRW)?

Non-Revenue Water represents water that is produced and lost before reaching the customer. It is calculated as:

```
NRW = System Input Volume - Billed Authorized Consumption
```

NRW is categorized into:

| Category | Description | Examples |
|----------|-------------|----------|
| **Real Losses** | Physical water losses | Pipe leaks, burst mains, tank overflows |
| **Apparent Losses** | Commercial losses | Meter errors, unauthorized consumption, data errors |
| **Unbilled Authorized** | Legitimate unbilled use | Fire fighting, flushing, public fountains |

### 2.2 IWA Water Balance

The system follows the International Water Association's standard water balance methodology:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        SYSTEM INPUT VOLUME                              â”‚
â”‚                              (SIV)                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚      AUTHORIZED CONSUMPTION     â”‚         NON-REVENUE WATER (NRW)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Billed         â”‚ Unbilled      â”‚ Apparent Losses â”‚    Real Losses      â”‚
â”‚  Metered        â”‚ Authorized    â”‚                 â”‚                     â”‚
â”‚  Consumption    â”‚ Consumption   â”‚ â€¢ Unauthorized  â”‚ â€¢ Leakage on mains  â”‚
â”‚                 â”‚               â”‚ â€¢ Meter errors  â”‚ â€¢ Leakage on        â”‚
â”‚                 â”‚ â€¢ Firefightingâ”‚ â€¢ Data handling â”‚   service pipes     â”‚
â”‚                 â”‚ â€¢ Flushing    â”‚   errors        â”‚ â€¢ Tank overflows    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.3 Performance Indicators

| Indicator | Formula | Target |
|-----------|---------|--------|
| **NRW %** | (SIV - Billed) / SIV Ã— 100 | < 25% |
| **ILI** | CARL / UARL | < 2.0 |
| **CARL** | Current Annual Real Losses | Minimize |
| **UARL** | Unavoidable Annual Real Losses | Baseline |

Where:
- **ILI** = Infrastructure Leakage Index
- **CARL** = Current Annual Real Losses (actual)
- **UARL** = Unavoidable Annual Real Losses (theoretical minimum)

---

## 3. Technology Stack

### 3.1 Complete Technology Matrix

| Layer | Technology | Version | Purpose |
|-------|------------|---------|---------|
| **Frontend** | Next.js | 14.0.4 | React framework with App Router |
| | React | 18.2.0 | UI component library |
| | TypeScript | 5.x | Type-safe JavaScript |
| | Tailwind CSS | 3.4.0 | Utility-first styling |
| | Recharts | 2.10.3 | Data visualization |
| | SWR | 2.2.4 | Data fetching & caching |
| | Lucide React | 0.303.0 | Icon library |
| | clsx | 2.1.0 | Conditional classnames |
| | date-fns | 3.2.0 | Date manipulation |
| **Backend** | Python | 3.10+ | Core programming language |
| | FastAPI | 0.104+ | High-performance REST API |
| | Uvicorn | 0.24+ | ASGI server |
| | Pydantic | 2.x | Data validation |
| **AI/ML** | scikit-learn | 1.3+ | Machine learning algorithms |
| | NumPy | 1.24+ | Numerical computing |
| | Pandas | 2.0+ | Data manipulation |
| | SciPy | 1.11+ | Scientific computing |
| **Database** | SQLite | 3.x | Development database |
| | PostgreSQL | 15+ | Production database |
| | TimescaleDB | 2.x | Time-series extension |
| **IoT** | MQTT | 5.0 | IoT messaging protocol |
| | Mosquitto | 2.x | MQTT broker |
| | ESP32 | - | Microcontroller |
| **Infrastructure** | Docker | 24+ | Containerization |
| | Docker Compose | 2.x | Multi-container orchestration |
| | Nginx | 1.25+ | Reverse proxy & load balancer |

### 3.2 Language Distribution

```
Python          â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘  45%
TypeScript      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  40%
CSS (Tailwind)  â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  10%
C++ (Arduino)   â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   3%
SQL             â–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘   2%
```

---

## 4. System Architecture

### 4.1 High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           PRESENTATION LAYER                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                    Next.js 14 Dashboard                            â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚
â”‚  â”‚  â”‚  Executive   â”‚ â”‚     DMA      â”‚ â”‚    Work      â”‚ â”‚   System   â”‚â”‚ â”‚
â”‚  â”‚  â”‚  Overview    â”‚ â”‚ Intelligence â”‚ â”‚   Orders     â”‚ â”‚   Health   â”‚â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                              HTTPS / WebSocket
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              API GATEWAY                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                         Nginx Reverse Proxy                        â”‚ â”‚
â”‚  â”‚            Load Balancing â€¢ SSL Termination â€¢ Rate Limiting        â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            SERVICE LAYER                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚    FastAPI Server   â”‚  â”‚   AI/ML Engine      â”‚  â”‚  Data Ingestion â”‚  â”‚
â”‚  â”‚                     â”‚  â”‚                     â”‚  â”‚    Service      â”‚  â”‚
â”‚  â”‚  â€¢ REST Endpoints   â”‚  â”‚  â€¢ Anomaly Detector â”‚  â”‚                 â”‚  â”‚
â”‚  â”‚  â€¢ Authentication   â”‚  â”‚  â€¢ Leak Localizer   â”‚  â”‚  â€¢ MQTT Client  â”‚  â”‚
â”‚  â”‚  â€¢ Rate Limiting    â”‚  â”‚  â€¢ Acoustic Analyzerâ”‚  â”‚  â€¢ Validation   â”‚  â”‚
â”‚  â”‚  â€¢ Response Cache   â”‚  â”‚  â€¢ Priority Scoring â”‚  â”‚  â€¢ Normalizationâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             DATA LAYER                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   PostgreSQL +      â”‚  â”‚     MQTT Broker     â”‚  â”‚   Redis Cache   â”‚  â”‚
â”‚  â”‚   TimescaleDB       â”‚  â”‚    (Mosquitto)      â”‚  â”‚                 â”‚  â”‚
â”‚  â”‚                     â”‚  â”‚                     â”‚  â”‚  â€¢ Session storeâ”‚  â”‚
â”‚  â”‚  â€¢ Sensor readings  â”‚  â”‚  â€¢ Pub/Sub topics   â”‚  â”‚  â€¢ Query cache  â”‚  â”‚
â”‚  â”‚  â€¢ DMA configs      â”‚  â”‚  â€¢ QoS levels       â”‚  â”‚  â€¢ Rate limitingâ”‚  â”‚
â”‚  â”‚  â€¢ Work orders      â”‚  â”‚  â€¢ Retained msgs    â”‚  â”‚                 â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                     â”‚
                                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           SENSOR NETWORK                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚  ESP32    â”‚  â”‚  ESP32    â”‚  â”‚  ESP32    â”‚  â”‚  ESP32    â”‚  ...       â”‚
â”‚  â”‚ Flow Meterâ”‚  â”‚ Pressure  â”‚  â”‚ Acoustic  â”‚  â”‚ Combined  â”‚            â”‚
â”‚  â”‚           â”‚  â”‚ Sensor    â”‚  â”‚ Sensor    â”‚  â”‚ Unit      â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Data Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          DATA FLOW SEQUENCE                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

1. SENSOR READING
   â”‚
   â”‚  ESP32 captures:
   â”‚  â€¢ Flow rate (pulses â†’ mÂ³/hour)
   â”‚  â€¢ Pressure (analog â†’ bar)
   â”‚  â€¢ Acoustic signature (microphone â†’ FFT)
   â”‚
   â–¼
2. MQTT PUBLISH
   â”‚
   â”‚  Topic: aquawatch/sensors/{sensor_id}/data
   â”‚  Payload: {
   â”‚    "sensor_id": "S-001",
   â”‚    "timestamp": "2026-01-17T10:30:00Z",
   â”‚    "flow_rate": 125.4,
   â”‚    "pressure": 3.2,
   â”‚    "battery": 87
   â”‚  }
   â”‚
   â–¼
3. DATA INGESTION
   â”‚
   â”‚  â€¢ Validate schema
   â”‚  â€¢ Check sensor status
   â”‚  â€¢ Normalize units
   â”‚  â€¢ Store to TimescaleDB
   â”‚
   â–¼
4. REAL-TIME ANALYSIS
   â”‚
   â”‚  AI Engine processes:
   â”‚  â€¢ Isolation Forest anomaly score
   â”‚  â€¢ Z-score deviation
   â”‚  â€¢ Pattern matching
   â”‚
   â–¼
5. ANOMALY DETECTED?
   â”‚
   â”œâ”€â”€ NO â†’ Continue monitoring
   â”‚
   â””â”€â”€ YES â†’ Trigger alert pipeline
              â”‚
              â–¼
6. LEAK LOCALIZATION
   â”‚
   â”‚  â€¢ Pressure gradient analysis
   â”‚  â€¢ Multi-sensor triangulation
   â”‚  â€¢ GPS coordinate estimation
   â”‚  â€¢ Confidence scoring
   â”‚
   â–¼
7. ALERT GENERATION
   â”‚
   â”‚  â€¢ Create work order
   â”‚  â€¢ Notify operators (WebSocket)
   â”‚  â€¢ Log to database
   â”‚  â€¢ Update dashboard KPIs
   â”‚
   â–¼
8. DASHBOARD UPDATE
   â”‚
   â”‚  SWR fetches new data
   â”‚  Components re-render
   â”‚  Real-time visualization
```

---

## 5. AI/ML Engine

### 5.1 Overview

The AI/ML Engine is the core intelligence of the system, responsible for:

- **Anomaly Detection** - Identifying unusual patterns in sensor data
- **Leak Localization** - Pinpointing the physical location of leaks
- **Acoustic Analysis** - Detecting leak signatures from sound patterns
- **Priority Scoring** - Ranking interventions by urgency and impact
- **Predictive Maintenance** - Forecasting potential failures

### 5.2 Anomaly Detection Module

**File:** `src/ai/anomaly_detector.py`

#### 5.2.1 Algorithm: Isolation Forest

Isolation Forest is an unsupervised machine learning algorithm that identifies anomalies by isolating observations in the feature space.

**How it works:**

1. Randomly select a feature
2. Randomly select a split value between min and max
3. Recursively partition data until each point is isolated
4. Anomalies require fewer splits to isolate (shorter path length)

**Mathematical Basis:**

```
Anomaly Score: s(x, n) = 2^(-E(h(x))/c(n))

Where:
- h(x) = path length of observation x
- E(h(x)) = average path length over all trees
- c(n) = average path length in unsuccessful search in BST
- n = number of samples
```

**Implementation:**

```python
from sklearn.ensemble import IsolationForest
import numpy as np

class AnomalyDetector:
    def __init__(self):
        self.model = IsolationForest(
            contamination=0.1,      # Expected 10% anomaly rate
            n_estimators=100,       # 100 trees for robust detection
            max_samples='auto',     # Samples per tree
            random_state=42,        # Reproducibility
            n_jobs=-1               # Parallel processing
        )
        self.is_fitted = False
    
    def fit(self, X: np.ndarray) -> None:
        """Train the model on historical normal data."""
        self.model.fit(X)
        self.is_fitted = True
    
    def predict(self, X: np.ndarray) -> np.ndarray:
        """
        Predict anomalies.
        Returns: 1 for normal, -1 for anomaly
        """
        if not self.is_fitted:
            raise ValueError("Model not fitted")
        return self.model.predict(X)
    
    def score_samples(self, X: np.ndarray) -> np.ndarray:
        """
        Get anomaly scores.
        More negative = more anomalous
        """
        return self.model.score_samples(X)
```

#### 5.2.2 Algorithm: Z-Score Analysis

Z-Score measures how many standard deviations a value is from the mean.

**Formula:**

```
Z = (X - Î¼) / Ïƒ

Where:
- X = observed value
- Î¼ = mean of the dataset
- Ïƒ = standard deviation
```

**Detection Rule:**

```python
def detect_zscore_anomaly(value: float, mean: float, std: float, threshold: float = 3.0) -> bool:
    """
    Values with |Z| > threshold are considered anomalies.
    Default threshold of 3.0 catches ~0.3% of normal distribution.
    """
    if std == 0:
        return False
    z_score = abs((value - mean) / std)
    return z_score > threshold
```

#### 5.2.3 Feature Engineering

The AI engine uses these features for detection:

| Feature | Description | Unit | Weight |
|---------|-------------|------|--------|
| `flow_rate` | Water flow measurement | mÂ³/hour | 0.35 |
| `pressure` | Line pressure | bar | 0.25 |
| `flow_variance` | Flow rate variability | mÂ³/hourÂ² | 0.15 |
| `pressure_variance` | Pressure variability | barÂ² | 0.10 |
| `hour_of_day` | Time-based patterns | 0-23 | 0.05 |
| `day_of_week` | Weekly patterns | 0-6 | 0.05 |
| `temperature` | Ambient temperature | Â°C | 0.05 |

**Feature Extraction Code:**

```python
import pandas as pd
import numpy as np

def extract_features(df: pd.DataFrame) -> pd.DataFrame:
    """Extract features for anomaly detection."""
    features = pd.DataFrame()
    
    # Base features
    features['flow_rate'] = df['flow_rate']
    features['pressure'] = df['pressure']
    
    # Statistical features (rolling window)
    window = 60  # 1 hour of minute data
    features['flow_mean'] = df['flow_rate'].rolling(window).mean()
    features['flow_std'] = df['flow_rate'].rolling(window).std()
    features['pressure_mean'] = df['pressure'].rolling(window).mean()
    features['pressure_std'] = df['pressure'].rolling(window).std()
    
    # Variance features
    features['flow_variance'] = features['flow_std'] ** 2
    features['pressure_variance'] = features['pressure_std'] ** 2
    
    # Time features
    features['hour'] = pd.to_datetime(df['timestamp']).dt.hour
    features['day_of_week'] = pd.to_datetime(df['timestamp']).dt.dayofweek
    
    # Derived features
    features['flow_pressure_ratio'] = features['flow_rate'] / (features['pressure'] + 0.1)
    
    return features.dropna()
```

### 5.3 Leak Localization Module

**File:** `src/ai/leak_localizer.py`

#### 5.3.1 Gradient-Based Localization

Leaks create pressure drops that propagate through the pipe network. By analyzing pressure gradients between multiple sensors, we can triangulate the leak location.

**Method:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PRESSURE GRADIENT ANALYSIS                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Sensor A (3.5 bar)          Sensor B (3.2 bar)         Sensor C (3.4 bar)
         â”‚                            â”‚                          â”‚
         â–¼                            â–¼                          â–¼
    â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•ªâ•â•â•â•â•
                                      â”‚
                                   LEAK ğŸ’§
                                   
    Pressure gradient: Î”P_AB = 0.3 bar, Î”P_BC = 0.2 bar
    Leak closer to B (highest pressure drop)
```

**Algorithm:**

```python
from dataclasses import dataclass
from typing import List, Tuple
import numpy as np

@dataclass
class SensorReading:
    sensor_id: str
    latitude: float
    longitude: float
    pressure: float
    flow_rate: float

@dataclass
class LeakLocation:
    latitude: float
    longitude: float
    confidence: float
    estimated_loss: float  # mÂ³/day

class LeakLocalizer:
    def __init__(self):
        self.baseline_pressures = {}
    
    def localize(self, readings: List[SensorReading]) -> LeakLocation:
        """
        Localize leak using weighted centroid method.
        Sensors with larger pressure drops have more weight.
        """
        if len(readings) < 3:
            raise ValueError("Need at least 3 sensors for triangulation")
        
        # Calculate pressure deviations
        deviations = []
        for r in readings:
            baseline = self.baseline_pressures.get(r.sensor_id, r.pressure)
            deviation = max(0, baseline - r.pressure)
            deviations.append(deviation)
        
        total_deviation = sum(deviations)
        if total_deviation == 0:
            # No leak detected
            return None
        
        # Weighted centroid calculation
        weights = [d / total_deviation for d in deviations]
        
        lat = sum(r.latitude * w for r, w in zip(readings, weights))
        lon = sum(r.longitude * w for r, w in zip(readings, weights))
        
        # Confidence based on deviation magnitude and sensor agreement
        confidence = min(100, sum(deviations) / len(deviations) * 100)
        
        # Estimate loss from flow imbalance
        estimated_loss = self._estimate_loss(readings)
        
        return LeakLocation(
            latitude=lat,
            longitude=lon,
            confidence=confidence,
            estimated_loss=estimated_loss
        )
    
    def _estimate_loss(self, readings: List[SensorReading]) -> float:
        """Estimate water loss in mÂ³/day from flow imbalance."""
        inflow = sum(r.flow_rate for r in readings if r.flow_rate > 0)
        outflow = abs(sum(r.flow_rate for r in readings if r.flow_rate < 0))
        imbalance = inflow - outflow
        return max(0, imbalance * 24)  # Convert hourly to daily
```

### 5.4 Acoustic Detection Module

**File:** `src/ai/acoustic_detection.py`

#### 5.4.1 FFT-Based Leak Signature Detection

Leaks produce characteristic acoustic signatures that can be detected using Fast Fourier Transform analysis.

**Leak Frequency Ranges:**

| Leak Type | Frequency Range | Amplitude Pattern |
|-----------|-----------------|-------------------|
| Small leak (< 1 L/min) | 500-1500 Hz | Low, consistent |
| Medium leak (1-10 L/min) | 200-800 Hz | Medium, pulsating |
| Large leak (> 10 L/min) | 50-300 Hz | High, irregular |
| Burst pipe | 20-100 Hz | Very high, decaying |

**Implementation:**

```python
import numpy as np
from scipy.fft import fft, fftfreq
from scipy.signal import butter, filtfilt

class AcousticDetector:
    def __init__(self, sample_rate: int = 44100):
        self.sample_rate = sample_rate
        self.leak_freq_ranges = [
            (50, 300),    # Large leaks
            (200, 800),   # Medium leaks
            (500, 1500),  # Small leaks
        ]
    
    def analyze(self, audio_samples: np.ndarray) -> dict:
        """
        Analyze acoustic signal for leak signatures.
        
        Returns:
            {
                'leak_detected': bool,
                'confidence': float (0-100),
                'leak_type': str,
                'dominant_frequency': float
            }
        """
        # Apply bandpass filter (20-2000 Hz for pipe acoustics)
        filtered = self._bandpass_filter(audio_samples, 20, 2000)
        
        # Compute FFT
        N = len(filtered)
        yf = fft(filtered)
        xf = fftfreq(N, 1 / self.sample_rate)
        
        # Get positive frequencies only
        positive_mask = xf >= 0
        frequencies = xf[positive_mask]
        magnitudes = np.abs(yf[positive_mask])
        
        # Analyze each leak frequency range
        leak_scores = []
        for low, high in self.leak_freq_ranges:
            mask = (frequencies >= low) & (frequencies <= high)
            if np.any(mask):
                energy = np.sum(magnitudes[mask] ** 2)
                leak_scores.append(energy)
            else:
                leak_scores.append(0)
        
        # Determine leak type and confidence
        max_score_idx = np.argmax(leak_scores)
        max_score = leak_scores[max_score_idx]
        
        # Normalize confidence (0-100)
        baseline_noise = np.median(magnitudes)
        confidence = min(100, (max_score / (baseline_noise + 1e-6)) * 10)
        
        leak_types = ['large', 'medium', 'small']
        
        # Find dominant frequency
        dominant_freq = frequencies[np.argmax(magnitudes)]
        
        return {
            'leak_detected': confidence > 60,
            'confidence': round(confidence, 1),
            'leak_type': leak_types[max_score_idx] if confidence > 60 else None,
            'dominant_frequency': round(dominant_freq, 1)
        }
    
    def _bandpass_filter(self, data: np.ndarray, low: float, high: float) -> np.ndarray:
        """Apply Butterworth bandpass filter."""
        nyquist = self.sample_rate / 2
        low_normalized = low / nyquist
        high_normalized = high / nyquist
        b, a = butter(4, [low_normalized, high_normalized], btype='band')
        return filtfilt(b, a, data)
```

### 5.5 Priority Scoring Algorithm

The system calculates a priority score (0-100) for each detected anomaly to help operators focus on the most critical issues.

**Scoring Formula:**

```
Priority Score = (Wâ‚ Ã— NRW_Impact) + (Wâ‚‚ Ã— Confidence) + (Wâ‚ƒ Ã— Duration) + (Wâ‚„ Ã— Location_Risk)

Where:
- Wâ‚ = 0.40 (NRW impact weight)
- Wâ‚‚ = 0.25 (Detection confidence weight)
- Wâ‚ƒ = 0.20 (Duration/persistence weight)
- Wâ‚„ = 0.15 (Location risk weight)
```

**Implementation:**

```python
@dataclass
class AnomalyPriority:
    anomaly_id: str
    priority_score: int  # 0-100
    nrw_impact: float    # Estimated mÂ³/day loss
    confidence: float    # Detection confidence %
    duration_hours: float
    location_risk: str   # 'high', 'medium', 'low'
    recommended_action: str

def calculate_priority(
    nrw_impact: float,      # mÂ³/day
    confidence: float,       # 0-100
    duration_hours: float,
    location_risk: str
) -> int:
    """Calculate priority score 0-100."""
    
    # Normalize NRW impact (assuming max 1000 mÂ³/day)
    nrw_score = min(100, (nrw_impact / 1000) * 100)
    
    # Duration score (longer = more urgent)
    duration_score = min(100, (duration_hours / 48) * 100)
    
    # Location risk multiplier
    risk_multipliers = {'high': 1.0, 'medium': 0.7, 'low': 0.4}
    location_score = risk_multipliers.get(location_risk, 0.5) * 100
    
    # Weighted sum
    score = (
        0.40 * nrw_score +
        0.25 * confidence +
        0.20 * duration_score +
        0.15 * location_score
    )
    
    return int(min(100, max(0, score)))
```

### 5.6 Model Training Pipeline

```python
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score
import joblib

class ModelTrainer:
    def __init__(self, model_path: str = 'models/anomaly_detector.joblib'):
        self.model_path = model_path
        self.detector = AnomalyDetector()
    
    def train(self, X: np.ndarray, y: np.ndarray = None) -> dict:
        """
        Train anomaly detection model.
        
        For unsupervised learning, y is not used but can be
        provided for evaluation if labeled data is available.
        """
        # Split data
        X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)
        
        # Train model
        self.detector.fit(X_train)
        
        # Evaluate
        predictions = self.detector.predict(X_test)
        
        metrics = {
            'training_samples': len(X_train),
            'test_samples': len(X_test),
            'anomaly_rate': (predictions == -1).mean()
        }
        
        # If labels provided, calculate supervised metrics
        if y is not None:
            y_train, y_test = train_test_split(y, test_size=0.2, random_state=42)
            y_pred = (predictions == -1).astype(int)
            y_true = y_test.astype(int)
            
            metrics['precision'] = precision_score(y_true, y_pred)
            metrics['recall'] = recall_score(y_true, y_pred)
            metrics['f1_score'] = f1_score(y_true, y_pred)
        
        # Save model
        joblib.dump(self.detector.model, self.model_path)
        metrics['model_saved'] = self.model_path
        
        return metrics
    
    def load(self) -> None:
        """Load trained model from disk."""
        self.detector.model = joblib.load(self.model_path)
        self.detector.is_fitted = True
```

---

## 6. Frontend Dashboard

### 6.1 Technology Overview

The dashboard is built with **Next.js 14** using the App Router paradigm, providing:

- **Server Components** - Default rendering on server for performance
- **Client Components** - Interactive elements with `'use client'` directive
- **File-based Routing** - Pages defined by folder structure
- **API Routes** - Backend API proxied through Next.js

### 6.2 Project Structure

```
dashboard/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app/                          # App Router pages
â”‚   â”‚   â”œâ”€â”€ layout.tsx                # Root layout with sidebar/topbar
â”‚   â”‚   â”œâ”€â”€ page.tsx                  # Executive Overview (/)
â”‚   â”‚   â”œâ”€â”€ globals.css               # Global styles & Tailwind
â”‚   â”‚   â”œâ”€â”€ dma/
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx              # DMA Intelligence (/dma)
â”‚   â”‚   â”‚   â””â”€â”€ [id]/
â”‚   â”‚   â”‚       â””â”€â”€ page.tsx          # DMA Deep Dive (/dma/[id])
â”‚   â”‚   â”œâ”€â”€ actions/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx              # Work Orders (/actions)
â”‚   â”‚   â””â”€â”€ health/
â”‚   â”‚       â””â”€â”€ page.tsx              # System Health (/health)
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx           # Navigation sidebar
â”‚   â”‚   â”‚   â””â”€â”€ TopBar.tsx            # Header with status
â”‚   â”‚   â”œâ”€â”€ metrics/
â”‚   â”‚   â”‚   â”œâ”€â”€ KPICard.tsx           # Key metric cards
â”‚   â”‚   â”‚   â””â”€â”€ StatusIndicators.tsx  # Status badges
â”‚   â”‚   â”œâ”€â”€ charts/
â”‚   â”‚   â”‚   â””â”€â”€ Charts.tsx            # Recharts wrappers
â”‚   â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”‚   â””â”€â”€ DataTable.tsx         # Data tables
â”‚   â”‚   â””â”€â”€ ui/
â”‚   â”‚       â””â”€â”€ Cards.tsx             # UI card components
â”‚   â””â”€â”€ lib/
â”‚       â””â”€â”€ api.ts                    # API client & hooks
â”œâ”€â”€ package.json
â”œâ”€â”€ tailwind.config.ts
â”œâ”€â”€ tsconfig.json
â””â”€â”€ next.config.js
```

### 6.3 Component Architecture

#### 6.3.1 KPICard Component

```tsx
// src/components/metrics/KPICard.tsx

interface KPICardProps {
  label: string
  value: string | number
  unit?: string
  trend?: 'up' | 'down' | 'stable'
  trendValue?: number
  status?: 'healthy' | 'warning' | 'critical'
  icon?: React.ReactNode
  sparkline?: number[]
}

export function KPICard({
  label,
  value,
  unit,
  trend,
  trendValue,
  status,
  icon,
  sparkline
}: KPICardProps) {
  // Gradient border based on status
  const getStatusBorder = () => {
    switch (status) {
      case 'healthy': return '#10b981'   // Emerald
      case 'warning': return '#f59e0b'   // Amber
      case 'critical': return '#ef4444'  // Red
      default: return '#3b82f6'          // Blue
    }
  }

  return (
    <div 
      className="card-premium p-5"
      style={{ borderTop: `3px solid ${getStatusBorder()}` }}
    >
      <div className="flex items-center gap-2 mb-3">
        {icon && <div className="w-9 h-9 rounded-xl...">{icon}</div>}
        <p className="text-xs font-semibold uppercase">{label}</p>
      </div>
      
      <div className="flex items-baseline gap-2">
        <span className="text-3xl font-bold">{value}</span>
        {unit && <span className="text-sm">{unit}</span>}
      </div>
      
      {sparkline && <MiniSparkline data={sparkline} />}
      
      {trend && <TrendBadge trend={trend} value={trendValue} />}
    </div>
  )
}
```

#### 6.3.2 Data Fetching with SWR

```tsx
// src/lib/api.ts

import useSWR from 'swr'

const API_BASE = '/api'

const fetcher = async (url: string) => {
  const res = await fetch(url)
  if (!res.ok) throw new Error('Failed to fetch')
  return res.json()
}

// Hook for system metrics
export function useSystemMetrics() {
  return useSWR(`${API_BASE}/metrics`, fetcher, {
    refreshInterval: 30000,  // Refresh every 30 seconds
    revalidateOnFocus: true
  })
}

// Hook for DMA list
export function useDMAList() {
  return useSWR(`${API_BASE}/dmas`, fetcher, {
    refreshInterval: 60000
  })
}

// Hook for individual DMA
export function useDMA(id: string) {
  return useSWR(id ? `${API_BASE}/dmas/${id}` : null, fetcher)
}

// Hook for leaks
export function useLeaks(dmaId?: string, status?: string) {
  const params = new URLSearchParams()
  if (dmaId) params.append('dma_id', dmaId)
  if (status) params.append('status', status)
  
  return useSWR(`${API_BASE}/leaks?${params}`, fetcher, {
    refreshInterval: 15000  // More frequent for alerts
  })
}
```

### 6.4 Styling System

#### 6.4.1 Tailwind Configuration

```typescript
// tailwind.config.ts

import type { Config } from 'tailwindcss'

const config: Config = {
  content: [
    './src/**/*.{js,ts,jsx,tsx,mdx}',
  ],
  theme: {
    extend: {
      colors: {
        // Custom semantic colors
        primary: {
          50: '#eff6ff',
          500: '#3b82f6',
          600: '#2563eb',
          700: '#1d4ed8',
        },
        status: {
          green: '#10b981',
          amber: '#f59e0b',
          red: '#ef4444',
        }
      },
      fontFamily: {
        sans: ['Inter', 'system-ui', 'sans-serif'],
        mono: ['JetBrains Mono', 'monospace'],
      },
      animation: {
        'pulse-live': 'pulse 2s cubic-bezier(0.4, 0, 0.6, 1) infinite',
      }
    },
  },
  plugins: [],
}

export default config
```

#### 6.4.2 Custom CSS Classes

```css
/* src/app/globals.css */

@tailwind base;
@tailwind components;
@tailwind utilities;

@layer components {
  /* Premium card with gradient border effect */
  .card-premium {
    @apply bg-white rounded-2xl shadow-lg relative overflow-hidden;
    background: linear-gradient(white, white) padding-box,
                linear-gradient(135deg, #e2e8f0, #f8fafc) border-box;
    border: 1px solid transparent;
  }
  
  .card-premium:hover {
    @apply shadow-xl;
  }
  
  /* Hero metric with gradient text */
  .hero-metric {
    @apply text-5xl font-extrabold tracking-tight;
    background: linear-gradient(135deg, #1e293b 0%, #3b82f6 100%);
    -webkit-background-clip: text;
    -webkit-text-fill-color: transparent;
  }
  
  /* Live indicator pulse */
  .pulse-live {
    animation: pulse-live 2s infinite;
  }
  
  @keyframes pulse-live {
    0%, 100% { opacity: 1; }
    50% { opacity: 0.5; }
  }
  
  /* Glass morphism effect */
  .glass {
    @apply bg-white/80 backdrop-blur-lg;
  }
}
```

### 6.5 Page Implementations

#### 6.5.1 Executive Overview Page

```tsx
// src/app/page.tsx (simplified)

'use client'

import { KPICard } from '@/components/metrics/KPICard'
import { NRWTrendChart } from '@/components/charts/Charts'
import { useSystemMetrics, useDMAList, useLeaks } from '@/lib/api'

export default function ExecutiveOverview() {
  const { data: metrics } = useSystemMetrics()
  const { data: dmas } = useDMAList()
  const { data: leaks } = useLeaks(undefined, 'detected')
  
  return (
    <div className="space-y-8">
      {/* Hero Section with dark gradient */}
      <div className="rounded-2xl bg-gradient-to-br from-slate-900 to-slate-800 p-8 text-white">
        <h1 className="text-4xl font-bold">Network Control Room</h1>
        {/* Hero stats grid */}
      </div>
      
      {/* KPI Cards */}
      <div className="grid grid-cols-4 gap-4">
        <KPICard
          label="Network NRW"
          value={metrics?.total_nrw_percent || 0}
          unit="%"
          trend="down"
          status={metrics?.total_nrw_percent > 35 ? 'critical' : 'healthy'}
        />
        {/* More KPIs */}
      </div>
      
      {/* Charts and tables */}
      <div className="grid grid-cols-3 gap-6">
        <div className="col-span-2">
          <NRWTrendChart data={trendData} />
        </div>
        <div>
          <AlertsList leaks={leaks} />
        </div>
      </div>
    </div>
  )
}
```

---

## 7. Backend API

### 7.1 API Structure

**File:** `src/api/integrated_api.py`

```python
from fastapi import FastAPI, HTTPException, Query
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
from datetime import datetime

app = FastAPI(
    title="AquaWatch NRW API",
    description="API for Non-Revenue Water Detection System",
    version="2.0.0"
)

# CORS for dashboard
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3001"],
    allow_methods=["*"],
    allow_headers=["*"],
)

# Pydantic models
class SystemMetrics(BaseModel):
    total_nrw_percent: float
    total_nrw_trend: str
    total_real_losses: float
    water_recovered_30d: float
    revenue_recovered_30d: float
    active_high_priority_leaks: int
    ai_status: str
    ai_confidence: float
    dma_count: int
    sensor_count: int
    last_data_received: datetime

class DMA(BaseModel):
    dma_id: str
    name: str
    nrw_percent: float
    priority_score: int
    status: str
    trend: str
    sensor_count: int
    area_km2: float
    population: int

class Leak(BaseModel):
    id: str
    dma_id: str
    location: str
    latitude: float
    longitude: float
    estimated_loss: float
    priority: str
    confidence: float
    status: str
    detected_at: datetime
    resolved_at: Optional[datetime]

# Endpoints
@app.get("/api/metrics", response_model=SystemMetrics)
async def get_system_metrics():
    """Get system-wide KPIs and metrics."""
    return calculate_system_metrics()

@app.get("/api/dmas", response_model=List[DMA])
async def get_dmas(
    sort_by: str = Query("priority_score", description="Sort field"),
    order: str = Query("desc", description="Sort order")
):
    """Get all DMAs with their current status."""
    return get_all_dmas(sort_by, order)

@app.get("/api/dmas/{dma_id}", response_model=DMA)
async def get_dma(dma_id: str):
    """Get detailed information for a specific DMA."""
    dma = get_dma_by_id(dma_id)
    if not dma:
        raise HTTPException(status_code=404, detail="DMA not found")
    return dma

@app.get("/api/leaks", response_model=List[Leak])
async def get_leaks(
    dma_id: Optional[str] = None,
    status: Optional[str] = None,
    priority: Optional[str] = None
):
    """Get detected leaks with optional filtering."""
    return get_filtered_leaks(dma_id, status, priority)

@app.get("/api/health")
async def health_check():
    """System health check endpoint."""
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow(),
        "components": {
            "api_server": {"status": "operational"},
            "database": {"status": "connected"},
            "ai_engine": {"status": "operational"},
            "mqtt_broker": {"status": "connected"}
        }
    }
```

### 7.2 API Endpoints Reference

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/metrics` | System-wide KPIs |
| GET | `/api/dmas` | List all DMAs |
| GET | `/api/dmas/{id}` | Single DMA details |
| GET | `/api/dmas/{id}/flow` | DMA flow time-series |
| GET | `/api/leaks` | List leaks (filterable) |
| POST | `/api/leaks/{id}/acknowledge` | Acknowledge leak |
| POST | `/api/leaks/{id}/resolve` | Mark leak resolved |
| GET | `/api/sensors` | List all sensors |
| GET | `/api/sensors/{id}` | Sensor details |
| GET | `/api/health` | System health check |
| GET | `/api/nrw/trend` | NRW trend data |

---

## 8. IoT & Sensor Integration

### 8.1 ESP32 Sensor Firmware

**File:** `firmware/aquawatch_sensor/aquawatch_sensor.ino`

```cpp
#include <WiFi.h>
#include <PubSubClient.h>
#include <ArduinoJson.h>

// Configuration
const char* WIFI_SSID = "WaterUtility_IoT";
const char* WIFI_PASSWORD = "secure_password";
const char* MQTT_SERVER = "mqtt.aquawatch.local";
const int MQTT_PORT = 1883;
const char* SENSOR_ID = "S-001";

// Pin definitions
#define FLOW_SENSOR_PIN 34
#define PRESSURE_SENSOR_PIN 35
#define LED_STATUS_PIN 2

// Calibration constants
const float FLOW_CALIBRATION = 7.5;  // Pulses per liter
const float PRESSURE_OFFSET = 0.5;   // Voltage offset
const float PRESSURE_SCALE = 1.2;    // Voltage to bar

WiFiClient espClient;
PubSubClient mqtt(espClient);

volatile int flowPulseCount = 0;
unsigned long lastPublish = 0;
const int PUBLISH_INTERVAL = 60000;  // 1 minute

void IRAM_ATTR flowPulseISR() {
    flowPulseCount++;
}

void setup() {
    Serial.begin(115200);
    
    pinMode(FLOW_SENSOR_PIN, INPUT_PULLUP);
    pinMode(PRESSURE_SENSOR_PIN, INPUT);
    pinMode(LED_STATUS_PIN, OUTPUT);
    
    attachInterrupt(digitalPinToInterrupt(FLOW_SENSOR_PIN), flowPulseISR, FALLING);
    
    connectWiFi();
    mqtt.setServer(MQTT_SERVER, MQTT_PORT);
}

void loop() {
    if (!mqtt.connected()) {
        reconnectMQTT();
    }
    mqtt.loop();
    
    if (millis() - lastPublish >= PUBLISH_INTERVAL) {
        publishSensorData();
        lastPublish = millis();
    }
}

void publishSensorData() {
    // Calculate flow rate (mÂ³/hour)
    float flowRate = (flowPulseCount / FLOW_CALIBRATION) * 60.0 / 1000.0;
    flowPulseCount = 0;
    
    // Read pressure (bar)
    int rawPressure = analogRead(PRESSURE_SENSOR_PIN);
    float voltage = rawPressure * (3.3 / 4095.0);
    float pressure = (voltage - PRESSURE_OFFSET) * PRESSURE_SCALE;
    
    // Create JSON payload
    StaticJsonDocument<256> doc;
    doc["sensor_id"] = SENSOR_ID;
    doc["timestamp"] = getISOTimestamp();
    doc["flow_rate"] = flowRate;
    doc["pressure"] = pressure;
    doc["battery"] = getBatteryPercent();
    doc["rssi"] = WiFi.RSSI();
    
    char payload[256];
    serializeJson(doc, payload);
    
    // Publish to MQTT
    String topic = String("aquawatch/sensors/") + SENSOR_ID + "/data";
    mqtt.publish(topic.c_str(), payload);
    
    digitalWrite(LED_STATUS_PIN, HIGH);
    delay(100);
    digitalWrite(LED_STATUS_PIN, LOW);
}
```

### 8.2 MQTT Topic Structure

```
aquawatch/
â”œâ”€â”€ sensors/
â”‚   â”œâ”€â”€ {sensor_id}/
â”‚   â”‚   â”œâ”€â”€ data          # Sensor readings (published by sensors)
â”‚   â”‚   â”œâ”€â”€ status        # Online/offline status
â”‚   â”‚   â””â”€â”€ config        # Configuration updates (subscribed by sensors)
â”œâ”€â”€ alerts/
â”‚   â”œâ”€â”€ leaks            # New leak detections
â”‚   â””â”€â”€ anomalies        # Anomaly alerts
â””â”€â”€ system/
    â”œâ”€â”€ health           # System health updates
    â””â”€â”€ commands         # System commands
```

### 8.3 Data Ingestion Service

```python
# src/iot/data_ingestion.py

import paho.mqtt.client as mqtt
import json
from datetime import datetime
from storage.database import DatabaseHandler
from ai.anomaly_detector import AnomalyDetector

class DataIngestionService:
    def __init__(self):
        self.db = DatabaseHandler()
        self.detector = AnomalyDetector()
        self.client = mqtt.Client()
        
        self.client.on_connect = self.on_connect
        self.client.on_message = self.on_message
    
    def start(self, broker_host: str, broker_port: int = 1883):
        """Start the ingestion service."""
        self.client.connect(broker_host, broker_port)
        self.client.loop_forever()
    
    def on_connect(self, client, userdata, flags, rc):
        """Subscribe to sensor data topics on connect."""
        print(f"Connected to MQTT broker with code {rc}")
        client.subscribe("aquawatch/sensors/+/data")
    
    def on_message(self, client, userdata, msg):
        """Process incoming sensor data."""
        try:
            payload = json.loads(msg.payload.decode())
            
            # Validate payload
            required_fields = ['sensor_id', 'timestamp', 'flow_rate', 'pressure']
            if not all(f in payload for f in required_fields):
                print(f"Invalid payload: missing fields")
                return
            
            # Store raw data
            self.db.store_sensor_reading(payload)
            
            # Run anomaly detection
            features = self._extract_features(payload)
            is_anomaly = self.detector.predict([features])[0] == -1
            
            if is_anomaly:
                self._handle_anomaly(payload, features)
        
        except json.JSONDecodeError:
            print(f"Invalid JSON in message")
        except Exception as e:
            print(f"Error processing message: {e}")
    
    def _handle_anomaly(self, payload: dict, features: list):
        """Handle detected anomaly."""
        alert = {
            'sensor_id': payload['sensor_id'],
            'timestamp': datetime.utcnow().isoformat(),
            'type': 'anomaly',
            'confidence': self.detector.get_confidence(features),
            'data': payload
        }
        
        # Publish alert
        self.client.publish(
            "aquawatch/alerts/anomalies",
            json.dumps(alert)
        )
        
        # Store alert
        self.db.store_alert(alert)
```

---

## 9. Data Models

### 9.1 Database Schema

```sql
-- Sensor readings (time-series)
CREATE TABLE sensor_readings (
    id SERIAL PRIMARY KEY,
    sensor_id VARCHAR(50) NOT NULL,
    timestamp TIMESTAMPTZ NOT NULL,
    flow_rate DECIMAL(10, 4),
    pressure DECIMAL(10, 4),
    temperature DECIMAL(5, 2),
    battery_percent INTEGER,
    rssi INTEGER,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Convert to hypertable for TimescaleDB
SELECT create_hypertable('sensor_readings', 'timestamp');

-- Index for queries
CREATE INDEX idx_sensor_readings_sensor_time 
ON sensor_readings (sensor_id, timestamp DESC);

-- DMAs (District Metered Areas)
CREATE TABLE dmas (
    dma_id VARCHAR(50) PRIMARY KEY,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    area_km2 DECIMAL(10, 2),
    population INTEGER,
    pipe_length_km DECIMAL(10, 2),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Sensors
CREATE TABLE sensors (
    sensor_id VARCHAR(50) PRIMARY KEY,
    dma_id VARCHAR(50) REFERENCES dmas(dma_id),
    name VARCHAR(255),
    type VARCHAR(50),  -- 'flow', 'pressure', 'acoustic', 'combined'
    latitude DECIMAL(10, 8),
    longitude DECIMAL(11, 8),
    installation_date DATE,
    last_calibration DATE,
    status VARCHAR(20) DEFAULT 'active',
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Detected leaks
CREATE TABLE leaks (
    id SERIAL PRIMARY KEY,
    leak_id VARCHAR(50) UNIQUE NOT NULL,
    dma_id VARCHAR(50) REFERENCES dmas(dma_id),
    latitude DECIMAL(10, 8),
    longitude DECIMAL(11, 8),
    location_description TEXT,
    estimated_loss_m3_day DECIMAL(10, 2),
    priority VARCHAR(20),  -- 'high', 'medium', 'low'
    confidence DECIMAL(5, 2),
    status VARCHAR(20) DEFAULT 'detected',  -- 'detected', 'confirmed', 'in_progress', 'resolved'
    detected_at TIMESTAMPTZ NOT NULL,
    confirmed_at TIMESTAMPTZ,
    resolved_at TIMESTAMPTZ,
    resolution_notes TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Work orders
CREATE TABLE work_orders (
    id SERIAL PRIMARY KEY,
    work_order_id VARCHAR(50) UNIQUE NOT NULL,
    leak_id VARCHAR(50) REFERENCES leaks(leak_id),
    assigned_to VARCHAR(255),
    priority VARCHAR(20),
    status VARCHAR(20) DEFAULT 'pending',
    scheduled_date DATE,
    completed_date DATE,
    notes TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Anomaly logs
CREATE TABLE anomaly_logs (
    id SERIAL PRIMARY KEY,
    sensor_id VARCHAR(50) REFERENCES sensors(sensor_id),
    timestamp TIMESTAMPTZ NOT NULL,
    anomaly_type VARCHAR(50),
    confidence DECIMAL(5, 2),
    features JSONB,
    resolved BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMPTZ DEFAULT NOW()
);
```

### 9.2 TypeScript Types

```typescript
// src/lib/types.ts

export interface SystemMetrics {
  total_nrw_percent: number
  total_nrw_trend: 'up' | 'down' | 'stable'
  total_real_losses: number
  water_recovered_30d: number
  revenue_recovered_30d: number
  active_high_priority_leaks: number
  ai_status: 'operational' | 'degraded' | 'offline'
  ai_confidence: number
  dma_count: number
  sensor_count: number
  last_data_received: string
}

export interface DMA {
  dma_id: string
  name: string
  nrw_percent: number
  priority_score: number
  status: 'healthy' | 'warning' | 'critical'
  trend: 'up' | 'down' | 'stable'
  sensor_count: number
  area_km2: number
  population: number
}

export interface Leak {
  id: string
  dma_id: string
  location: string
  latitude: number
  longitude: number
  estimated_loss: number
  priority: 'high' | 'medium' | 'low'
  confidence: number
  status: 'detected' | 'confirmed' | 'in_progress' | 'resolved'
  detected_at: string
  resolved_at?: string
}

export interface Sensor {
  sensor_id: string
  dma_id: string
  name: string
  type: 'flow' | 'pressure' | 'acoustic' | 'combined'
  latitude: number
  longitude: number
  status: 'active' | 'inactive' | 'maintenance'
  last_reading?: {
    timestamp: string
    flow_rate?: number
    pressure?: number
  }
}

export interface WorkOrder {
  id: string
  leak_id: string
  assigned_to: string
  priority: 'high' | 'medium' | 'low'
  status: 'pending' | 'in_progress' | 'completed' | 'cancelled'
  scheduled_date: string
  completed_date?: string
}
```

---

## 10. Deployment

### 10.1 Docker Compose Configuration

```yaml
# docker-compose.yml

version: '3.8'

services:
  # API Server
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://aquawatch:password@db:5432/aquawatch
      - MQTT_BROKER=mqtt
      - AI_MODEL_PATH=/app/models
    depends_on:
      - db
      - mqtt
    volumes:
      - ./models:/app/models
    restart: unless-stopped

  # Dashboard
  dashboard:
    build:
      context: .
      dockerfile: Dockerfile.dashboard
    ports:
      - "3001:3001"
    environment:
      - API_URL=http://api:8000
    depends_on:
      - api
    restart: unless-stopped

  # Data Ingestion Service
  ingestion:
    build:
      context: .
      dockerfile: Dockerfile.ingestion
    environment:
      - DATABASE_URL=postgresql://aquawatch:password@db:5432/aquawatch
      - MQTT_BROKER=mqtt
    depends_on:
      - db
      - mqtt
    restart: unless-stopped

  # Database
  db:
    image: timescale/timescaledb:latest-pg15
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=aquawatch
      - POSTGRES_PASSWORD=password
      - POSTGRES_DB=aquawatch
    volumes:
      - pgdata:/var/lib/postgresql/data
    restart: unless-stopped

  # MQTT Broker
  mqtt:
    image: eclipse-mosquitto:2
    ports:
      - "1883:1883"
      - "9001:9001"
    volumes:
      - ./mosquitto/mosquitto.conf:/mosquitto/config/mosquitto.conf
      - mqttdata:/mosquitto/data
    restart: unless-stopped

  # Nginx Reverse Proxy
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
      - ./nginx/ssl:/etc/nginx/ssl
    depends_on:
      - api
      - dashboard
    restart: unless-stopped

volumes:
  pgdata:
  mqttdata:
```

### 10.2 Production Deployment Steps

```bash
# 1. Clone repository
git clone https://github.com/aquawatch/nrw-detection-system.git
cd nrw-detection-system

# 2. Configure environment
cp .env.example .env
# Edit .env with production values

# 3. Build and start services
docker-compose -f docker-compose.prod.yml up -d --build

# 4. Run database migrations
docker-compose exec api python -m alembic upgrade head

# 5. Train initial AI model (if needed)
docker-compose exec api python scripts/train_model.py

# 6. Verify deployment
curl http://localhost/api/health
```

---

## 11. API Reference

### 11.1 Authentication

All API endpoints require authentication via JWT token:

```http
Authorization: Bearer <token>
```

### 11.2 Endpoints

#### GET /api/metrics

**Response:**

```json
{
  "total_nrw_percent": 32.4,
  "total_nrw_trend": "down",
  "total_real_losses": 12450,
  "water_recovered_30d": 8230,
  "revenue_recovered_30d": 485000,
  "active_high_priority_leaks": 3,
  "ai_status": "operational",
  "ai_confidence": 94,
  "dma_count": 12,
  "sensor_count": 48,
  "last_data_received": "2026-01-17T10:30:00Z"
}
```

#### GET /api/dmas

**Query Parameters:**

| Parameter | Type | Description |
|-----------|------|-------------|
| sort_by | string | Field to sort by (default: priority_score) |
| order | string | Sort order: asc/desc (default: desc) |

**Response:**

```json
[
  {
    "dma_id": "dma-001",
    "name": "Kabulonga North",
    "nrw_percent": 45.2,
    "priority_score": 87,
    "status": "critical",
    "trend": "up",
    "sensor_count": 8,
    "area_km2": 4.5,
    "population": 25000
  }
]
```

#### GET /api/leaks

**Query Parameters:**

| Parameter | Type | Description |
|-----------|------|-------------|
| dma_id | string | Filter by DMA |
| status | string | Filter by status |
| priority | string | Filter by priority |

**Response:**

```json
[
  {
    "id": "leak-001",
    "dma_id": "dma-001",
    "location": "Junction Rd & Main St",
    "latitude": -15.4167,
    "longitude": 28.2833,
    "estimated_loss": 450,
    "priority": "high",
    "confidence": 92,
    "status": "detected",
    "detected_at": "2026-01-17T09:15:00Z"
  }
]
```

---

## 12. Appendices

### 12.1 Glossary

| Term | Definition |
|------|------------|
| **NRW** | Non-Revenue Water - water produced but not billed |
| **DMA** | District Metered Area - hydraulically isolated zone |
| **MNF** | Minimum Night Flow - lowest flow (typically 2-4 AM) |
| **ILI** | Infrastructure Leakage Index - IWA performance metric |
| **CARL** | Current Annual Real Losses |
| **UARL** | Unavoidable Annual Real Losses |
| **FFT** | Fast Fourier Transform - signal analysis algorithm |

### 12.2 References

1. IWA Water Loss Task Force. "Best Practice Performance Indicators"
2. Lambert, A. "International Report on Water Losses Management"
3. scikit-learn Documentation: Isolation Forest
4. Next.js 14 Documentation
5. TimescaleDB Best Practices Guide

### 12.3 Version History

| Version | Date | Changes |
|---------|------|---------|
| 2.0 | Jan 2026 | Next.js dashboard, enhanced AI |
| 1.5 | Nov 2025 | Acoustic detection module |
| 1.0 | Sep 2025 | Initial release |

---

**Â© 2026 AquaWatch Technologies. All Rights Reserved.**
